apiVersion: apps/v1
kind: Deployment
metadata:
  name: sd15-api
  namespace: sd15-api
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels: { app: sd15-api }
  template:
    metadata:
      labels: { app: sd15-api }
    spec:
      runtimeClassName: nvidia
      # Optional: pin to a specific GPU node
      # nodeSelector: { gpu: "1070" }
      containers:
      - name: api
        image: pytorch/pytorch:2.3.1-cuda11.8-cudnn8-runtime
        imagePullPolicy: IfNotPresent
        command: ["bash","-lc"]
        args:
          - |
            set -euo pipefail
            cd /app
            export PIP_NO_CACHE_DIR=${PIP_NO_CACHE_DIR:-1}
            REQ_HASH=$(sha256sum requirements.txt | awk '{print $1}')
            STAMP_DIR=/models/deps
            STAMP_FILE="${STAMP_DIR}/.pip-${REQ_HASH}"
            mkdir -p "${STAMP_DIR}"
            if [ ! -f "${STAMP_FILE}" ]; then
              echo "Installing python dependencies (hash=${REQ_HASH})"
              pip install --no-cache-dir -r requirements.txt
              find "${STAMP_DIR}" -maxdepth 1 -type f -name '.pip-*' -not -name ".pip-${REQ_HASH}" -delete || true
              touch "${STAMP_FILE}"
            else
              echo "Python dependencies already installed for ${REQ_HASH}, skipping pip install"
            fi
            python -m uvicorn app:app --host 0.0.0.0 --port 8000
        env:
          - { name: NVIDIA_VISIBLE_DEVICES, value: "all" }
          - { name: NVIDIA_DRIVER_CAPABILITIES, value: "compute,utility" }
          - { name: MODEL_ID, value: "runwayml/stable-diffusion-v1-5" }
          # Flip to true on the 1060 if you hit OOM
          - { name: VAE_CPU, value: "false" }
          - { name: TRANSFORMERS_CACHE, value: "/models/.cache/huggingface" }
          - { name: HF_HOME, value: "/models/.cache/huggingface" }
          - { name: PYTORCH_CUDA_ALLOC_CONF, value: "max_split_size_mb:128" }
          - { name: HF_HUB_DISABLE_SYMLINKS_WARNING, value: "1" }
        volumeMounts:
          - { name: app, mountPath: /app, readOnly: true }
          - { name: models, mountPath: /models }
        ports:
          - { name: http, containerPort: 8000 }
        resources:
          requests:
            cpu: "500m"
            memory: "4Gi"
          limits:
            cpu: "2"
            memory: "10Gi"
            nvidia.com/gpu: 1
        readinessProbe:
          httpGet: { path: /healthz, port: 8000 }
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
        - name: app
          configMap:
            name: sd15-app
            defaultMode: 0444
        - name: models
          persistentVolumeClaim:
            claimName: sd15-cache
