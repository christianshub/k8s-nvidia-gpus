apiVersion: v1
kind: ConfigMap
metadata:
  name: sd15-app
  namespace: sd15-api
data:
  requirements.txt: |
    fastapi==0.115.0
    uvicorn[standard]==0.30.6
    torch==2.3.1
    torchvision==0.18.1
    transformers==4.44.2
    diffusers==0.30.2
    accelerate==0.34.2
    safetensors==0.4.4
  app.py: |
    import base64
    import io
    import logging
    import os
    import time
    from contextlib import nullcontext
    from threading import Lock
    import torch
    from fastapi import FastAPI, HTTPException
    from pydantic import BaseModel
    from fastapi.responses import HTMLResponse, Response
    from diffusers import StableDiffusionPipeline

    MODEL_ID = os.getenv("MODEL_ID", "runwayml/stable-diffusion-v1-5")
    TORCH_DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger("sd15-api")
    logger.setLevel(logging.INFO)

    app = FastAPI(title="SD15 Minimal API")
    _LAST_IMAGE: bytes | None = None
    _LAST_LOCK = Lock()

    pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=TORCH_DTYPE)
    pipe.enable_attention_slicing()
    pipe.enable_vae_slicing()
    if os.getenv("VAE_CPU", "false").lower() in ("1","true","yes"):
        pipe.vae.to("cpu")
    if torch.cuda.is_available():
        pipe = pipe.to("cuda")
        logger.info("Loaded %s on CUDA (%s)", MODEL_ID, TORCH_DTYPE)
    else:
        logger.warning("CUDA unavailable, running %s on CPU", MODEL_ID)

    class GenReq(BaseModel):
        prompt: str
        steps: int | None = 30
        guidance_scale: float | None = 7.5
        seed: int | None = None
        width: int | None = 512
        height: int | None = 512

    @app.get("/healthz")
    def healthz():
        return {"ok": True}

    @app.get("/", response_class=HTMLResponse)
    def index():
        if _LAST_IMAGE is None:
            return "<h1>SD1.5 GPU API</h1><p>No image generated yet. POST /generate to create one.</p>"
        preview = base64.b64encode(_LAST_IMAGE).decode("ascii")
        return f"""
        <html>
          <head><title>SD1.5 Demo</title></head>
          <body style="background:#0b0b0f;color:#f0f0f0;font-family:sans-serif;">
            <h1>Latest image</h1>
            <img src="data:image/png;base64,{preview}" alt="latest image" style="max-width:90vw;height:auto;border:3px solid #333;border-radius:8px;" />
            <p>POST <code>/generate</code> with a prompt to update this preview.</p>
          </body>
        </html>
        """

    @app.get("/last")
    def last_image():
        if _LAST_IMAGE is None:
            raise HTTPException(404, "No image generated yet")
        return Response(content=_LAST_IMAGE, media_type="image/png")

    @app.post("/generate")
    def generate(req: GenReq):
        if not req.prompt or len(req.prompt.strip()) == 0:
            raise HTTPException(400, "prompt is required")
        gen = None
        if req.seed is not None:
            gen = torch.Generator(device="cuda" if torch.cuda.is_available() else "cpu").manual_seed(req.seed)
        t0 = time.time()
        logger.info(
            "Generating prompt='%s' steps=%s guidance=%.2f seed=%s size=%sx%s",
            req.prompt,
            req.steps or 30,
            req.guidance_scale or 7.5,
            req.seed or "auto",
            req.width or 512,
            req.height or 512,
        )
        precision = torch.autocast(device_type="cuda", dtype=TORCH_DTYPE) if torch.cuda.is_available() else nullcontext()
        with precision:
            image = pipe(
                req.prompt,
                num_inference_steps=req.steps or 30,
                guidance_scale=req.guidance_scale or 7.5,
                width=req.width or 512,
                height=req.height or 512,
                generator=gen
        ).images[0]
        buf = io.BytesIO()
        image.save(buf, format="PNG")
        latency = time.time() - t0
        logger.info("Completed generation in %.2fs", latency)
        png = buf.getvalue()
        global _LAST_IMAGE
        with _LAST_LOCK:
            _LAST_IMAGE = png
        return Response(content=png, media_type="image/png", headers={"X-Gen-Time": f"{latency:.2f}s"})
