apiVersion: apps/v1
kind: Deployment
metadata:
  name: coder-llm
  namespace: llm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  selector:
    matchLabels:
      app: coder-llm
  template:
    metadata:
      labels:
        app: coder-llm
    spec:
      runtimeClassName: nvidia
      initContainers:
        - name: fetch-model
          image: curlimages/curl:8.7.1
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          command: ["/bin/sh","-lc"]
          env:
            - name: MODEL_FILE
              value: qwen2.5-7b-instruct-abliterated-v2.Q4_K_M.gguf
            - name: MODEL_URL
              value: https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-abliterated-v2-GGUF/resolve/main/Qwen2.5-7B-Instruct-abliterated-v2.Q4_K_M.gguf
          args:
            - |
              set -eu
              mkdir -p /models
              
              # FIX: If the file is smaller than 10MB, it's a fake HTML error page. Delete it.
              if [ -f "/models/${MODEL_FILE}" ]; then
                FILE_SIZE=$(stat -c%s "/models/${MODEL_FILE}")
                if [ "$FILE_SIZE" -lt 10000000 ]; then
                  echo "Detected invalid small file (size: $FILE_SIZE). Deleting to redownload..."
                  rm "/models/${MODEL_FILE}"
                fi
              fi

              if [ ! -s "/models/${MODEL_FILE}" ]; then
                echo "Downloading ${MODEL_FILE}..."
                # Added -f to fail the container if the URL returns a 404
                curl -fL --retry 5 --retry-delay 5 -o "/models/${MODEL_FILE}.tmp" "${MODEL_URL}"
                mv "/models/${MODEL_FILE}.tmp" "/models/${MODEL_FILE}"
              else
                echo "Model already exists and verified."
              fi
          volumeMounts:
            - name: models
              mountPath: /models
      containers:
        - name: server
          image: ghcr.io/ggml-org/llama.cpp:server-cuda
          imagePullPolicy: IfNotPresent
          command: ["/bin/sh","-lc"]
          env:
            - name: MODEL_FILE
              value: qwen2.5-7b-instruct-abliterated-v2.Q4_K_M.gguf
            - name: CTX_SIZE
              value: "4096"
            - name: GPU_LAYERS
              value: "35" 
            - name: CPU_THREADS
              value: "6"
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          args:
            - |
              set -eu
              exec /app/llama-server \
                -m "/models/${MODEL_FILE}" \
                --host 0.0.0.0 \
                --port 8080 \
                --ctx-size "${CTX_SIZE}" \
                --n-gpu-layers "${GPU_LAYERS}" \
                --threads "${CPU_THREADS}"
          ports:
            - name: http
              containerPort: 8080
          resources:
            limits:
              nvidia.com/gpu: 1
              memory: 10Gi
              cpu: "4"
            requests:
              memory: 6Gi
              cpu: "2"
          volumeMounts:
            - name: models
              mountPath: /models
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llm-models