apiVersion: apps/v1
kind: Deployment
metadata:
  name: coder-llm
  namespace: llm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  selector:
    matchLabels:
      app: coder-llm
  template:
    metadata:
      labels:
        app: coder-llm
    spec:
      runtimeClassName: nvidia
      initContainers:
        - name: fetch-model
          image: curlimages/curl:8.7.1
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          command: ["/bin/sh","-lc"]
          env:
            - name: MODEL_FILE
              value: qwen2.5-7b-instruct-abliterated-v2.Q4_K_M.gguf
            - name: MODEL_URL
              value: https://huggingface.co/QuantFactory/Qwen2.5-7B-Instruct-abliterated-v2-GGUF/resolve/main/qwen2.5-7b-instruct-abliterated-v2.Q4_K_M.gguf
          args:
            - |
              set -eu
              mkdir -p /models
              if [ ! -s "/models/${MODEL_FILE}" ]; then
                echo "Downloading ${MODEL_FILE}..."
                curl -L --retry 5 --retry-delay 5 -o "/models/${MODEL_FILE}.tmp" "${MODEL_URL}"
                mv "/models/${MODEL_FILE}.tmp" "/models/${MODEL_FILE}"
              fi
          volumeMounts:
            - name: models
              mountPath: /models
      containers:
        - name: server
          image: ghcr.io/ggml-org/llama.cpp:server-cuda
          imagePullPolicy: IfNotPresent
          command: ["/bin/sh","-lc"]
          env:
            - name: MODEL_FILE
              value: qwen2.5-7b-instruct-abliterated-v2.Q4_K_M.gguf
            - name: CTX_SIZE
              value: "4096" # Qwen2.5 handles large context well; bumped slightly from 2048
            - name: GPU_LAYERS
              value: "35" # Setting this higher than 28 ensures 100% GPU offload
            - name: CPU_THREADS
              value: "6"
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          args:
            - |
              set -eu
              exec /app/llama-server \
                -m "/models/${MODEL_FILE}" \
                --host 0.0.0.0 \
                --port 8080 \
                --ctx-size "${CTX_SIZE}" \
                --n-gpu-layers "${GPU_LAYERS}" \
                --threads "${CPU_THREADS}"
          ports:
            - name: http
              containerPort: 8080
          resources:
            limits:
              nvidia.com/gpu: 1
              memory: 10Gi # Reduced slightly as 12Gi is plenty
              cpu: "4"
            requests:
              memory: 6Gi
              cpu: "2"
          volumeMounts:
            - name: models
              mountPath: /models
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llm-models